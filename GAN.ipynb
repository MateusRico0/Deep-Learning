{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dimension):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dimension, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dimension, img_dimension):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(noise_dimension, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, img_dimension),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, noise_dimension, img_dimension):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(noise_dimension, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, img_dimension),\n",
    "            nn.Tanh() # Queremos que os valores das imagens esteja entre -1 e 1, porque o mnist est√° entre -1 e 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4 \n",
    "noise_dim = 64\n",
    "image_dim = 28 * 28 * 1\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
    "loader = DataLoader(df, batch_size=batch_size, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model load and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(noise_dim, image_dim).to(device)\n",
    "\n",
    "fixed_noise = torch.randn((batch_size, noise_dim)).to(device) \n",
    "\n",
    "transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "writer_fake = SummaryWriter(f'runs/GAN_MNIST/fake')\n",
    "writer_real = SummaryWriter(f'runs/GAN_MNIST/real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/1875                       Loss D: 0.6427, loss G: 0.6828\n",
      "Epoch [1/50] Batch 0/1875                       Loss D: 0.3440, loss G: 1.3385\n",
      "Epoch [2/50] Batch 0/1875                       Loss D: 0.4925, loss G: 1.3491\n",
      "Epoch [3/50] Batch 0/1875                       Loss D: 0.5468, loss G: 1.0681\n",
      "Epoch [4/50] Batch 0/1875                       Loss D: 0.5967, loss G: 1.1499\n",
      "Epoch [5/50] Batch 0/1875                       Loss D: 0.6295, loss G: 1.0062\n",
      "Epoch [6/50] Batch 0/1875                       Loss D: 0.6767, loss G: 0.9817\n",
      "Epoch [7/50] Batch 0/1875                       Loss D: 1.1963, loss G: 0.6574\n",
      "Epoch [8/50] Batch 0/1875                       Loss D: 0.8996, loss G: 0.7390\n",
      "Epoch [9/50] Batch 0/1875                       Loss D: 0.6270, loss G: 1.1691\n",
      "Epoch [10/50] Batch 0/1875                       Loss D: 0.5382, loss G: 1.0862\n",
      "Epoch [11/50] Batch 0/1875                       Loss D: 0.6608, loss G: 1.0290\n",
      "Epoch [12/50] Batch 0/1875                       Loss D: 0.7542, loss G: 0.9345\n",
      "Epoch [13/50] Batch 0/1875                       Loss D: 0.7236, loss G: 0.7373\n",
      "Epoch [14/50] Batch 0/1875                       Loss D: 0.5091, loss G: 1.1513\n",
      "Epoch [15/50] Batch 0/1875                       Loss D: 0.8804, loss G: 0.8460\n",
      "Epoch [16/50] Batch 0/1875                       Loss D: 0.8096, loss G: 0.6316\n",
      "Epoch [17/50] Batch 0/1875                       Loss D: 0.6837, loss G: 1.1032\n",
      "Epoch [18/50] Batch 0/1875                       Loss D: 0.5212, loss G: 1.4030\n",
      "Epoch [19/50] Batch 0/1875                       Loss D: 0.5399, loss G: 1.0742\n",
      "Epoch [20/50] Batch 0/1875                       Loss D: 0.7332, loss G: 0.8360\n",
      "Epoch [21/50] Batch 0/1875                       Loss D: 0.6329, loss G: 1.2036\n",
      "Epoch [22/50] Batch 0/1875                       Loss D: 0.5070, loss G: 1.0537\n",
      "Epoch [23/50] Batch 0/1875                       Loss D: 0.6228, loss G: 1.1954\n",
      "Epoch [24/50] Batch 0/1875                       Loss D: 0.9616, loss G: 0.7394\n",
      "Epoch [25/50] Batch 0/1875                       Loss D: 0.6456, loss G: 0.7586\n",
      "Epoch [26/50] Batch 0/1875                       Loss D: 0.5780, loss G: 1.1362\n",
      "Epoch [27/50] Batch 0/1875                       Loss D: 0.6960, loss G: 1.1544\n",
      "Epoch [28/50] Batch 0/1875                       Loss D: 0.5977, loss G: 0.9944\n",
      "Epoch [29/50] Batch 0/1875                       Loss D: 0.6964, loss G: 0.9125\n",
      "Epoch [30/50] Batch 0/1875                       Loss D: 0.5665, loss G: 0.9492\n",
      "Epoch [31/50] Batch 0/1875                       Loss D: 0.6600, loss G: 0.8831\n",
      "Epoch [32/50] Batch 0/1875                       Loss D: 0.6096, loss G: 1.0263\n",
      "Epoch [33/50] Batch 0/1875                       Loss D: 0.6788, loss G: 1.0759\n",
      "Epoch [34/50] Batch 0/1875                       Loss D: 0.6752, loss G: 0.7406\n",
      "Epoch [35/50] Batch 0/1875                       Loss D: 0.6425, loss G: 1.1657\n",
      "Epoch [36/50] Batch 0/1875                       Loss D: 0.7067, loss G: 0.8763\n",
      "Epoch [37/50] Batch 0/1875                       Loss D: 0.6803, loss G: 0.9696\n",
      "Epoch [38/50] Batch 0/1875                       Loss D: 0.5206, loss G: 1.0810\n",
      "Epoch [39/50] Batch 0/1875                       Loss D: 0.6329, loss G: 0.8551\n",
      "Epoch [40/50] Batch 0/1875                       Loss D: 0.4939, loss G: 1.2390\n",
      "Epoch [41/50] Batch 0/1875                       Loss D: 0.5609, loss G: 0.8609\n",
      "Epoch [42/50] Batch 0/1875                       Loss D: 0.5777, loss G: 0.9588\n",
      "Epoch [43/50] Batch 0/1875                       Loss D: 0.5476, loss G: 1.1330\n",
      "Epoch [44/50] Batch 0/1875                       Loss D: 0.7238, loss G: 0.8803\n",
      "Epoch [45/50] Batch 0/1875                       Loss D: 0.6640, loss G: 0.8284\n",
      "Epoch [46/50] Batch 0/1875                       Loss D: 0.6248, loss G: 1.0732\n",
      "Epoch [47/50] Batch 0/1875                       Loss D: 0.5561, loss G: 0.9598\n",
      "Epoch [48/50] Batch 0/1875                       Loss D: 0.6189, loss G: 0.9445\n",
      "Epoch [49/50] Batch 0/1875                       Loss D: 0.5978, loss G: 0.9290\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.view(-1, 784).to(device) # flatten\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ## Generates the noise\n",
    "        noise = torch.randn(batch_size, noise_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ## Discriminator\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "        disc_fake = disc(fake.detach()).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "\n",
    "        disc.zero_grad()\n",
    "        lossD.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ## Genator\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
